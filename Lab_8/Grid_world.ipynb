{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "17ecc99b"
      },
      "source": [
        "wall = [(1,1)]\n",
        "terminal_states = [(1,3),(2,3)]\n",
        "\n",
        "# possible actions (equally probable)\n",
        "action_probability = {'L':0.25,'R':0.25,'U':0.25,'D':0.25}\n",
        "\n",
        "# stochastic environment actions\n",
        "environment_left = {'L':'D','R':'U','U':'L','D':'R'}\n",
        "environment_right = {'L':'U','R':'D','U':'R','D':'L'}"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e2c330d3"
      },
      "source": [
        "# cell is available or not\n",
        "def is_valid(i,j):\n",
        "    return (i,j) not in wall and i >= 0 and i < 3 and j >= 0 and j < 4\n",
        "\n",
        "#take action\n",
        "def transition(action,i,j):\n",
        "    if action == 'L':\n",
        "        return (i,j-1)\n",
        "    elif action == 'R':\n",
        "        return (i,j+1)\n",
        "    elif action == 'U':\n",
        "        return (i+1,j)\n",
        "    elif action == 'D':\n",
        "        return (i-1,j)\n",
        "    else:\n",
        "        return (-1,-1)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8c81b1e1"
      },
      "source": [
        "# value function\n",
        "def value_function(i,j,reward,reward_matrix,discount_factor,V_pie):\n",
        "    value = 0\n",
        "    for action in ['L','R','U','D']:\n",
        "        # desired action with 0.8 probability\n",
        "        state_x,state_y = transition(action,i,j)\n",
        "        if is_valid(state_x,state_y):\n",
        "            desired_action_value = (reward_matrix[state_x][state_y] + discount_factor*V_pie[state_x][state_y])\n",
        "        else:\n",
        "            desired_action_value = (reward_matrix[i][j] + discount_factor*V_pie[i][j])\n",
        "\n",
        "        # environment action with 0.1 probability\n",
        "        state_x,state_y = transition(environment_left[action],i,j)\n",
        "        if is_valid(state_x,state_y):\n",
        "            env_action_left_value = (reward_matrix[state_x][state_y] + discount_factor*V_pie[state_x][state_y])\n",
        "        else:\n",
        "            env_action_left_value = (reward_matrix[i][j] + discount_factor*V_pie[i][j])\n",
        "\n",
        "        # environment action with 0.1 probability\n",
        "        state_x,state_y = transition(environment_right[action],i,j)\n",
        "        if is_valid(state_x,state_y):\n",
        "            env_action_right_value = (reward_matrix[state_x][state_y] + discount_factor*V_pie[state_x][state_y])\n",
        "        else:\n",
        "            env_action_right_value = (reward_matrix[i][j] + discount_factor*V_pie[i][j])\n",
        "\n",
        "        value_to_action = desired_action_value*0.8+env_action_left_value*0.1+env_action_right_value*0.1\n",
        "\n",
        "        value += value_to_action*0.25\n",
        "\n",
        "    return value"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0395d552"
      },
      "source": [
        "# iterative policy evaluation\n",
        "def iterative_policy_evaluation(iter,epsilon,reward,reward_matrix,V_pie,discount_factor):\n",
        "    while True:\n",
        "        delta = 0\n",
        "        for i in range(3):\n",
        "            for j in range(4):\n",
        "                state = (i,j)\n",
        "                if state in terminal_states or state in wall:  # continue if encounter terminal state or wall\n",
        "                    continue\n",
        "                v = V_pie[i][j]\n",
        "                V_pie[i][j] = value_function(i,j,reward,reward_matrix,discount_factor,V_pie)\n",
        "                delta = max(delta,abs(v-V_pie[i][j]))\n",
        "        iter += 1\n",
        "        if delta < epsilon:\n",
        "            print(f\"Number of iterations = {iter}\")\n",
        "            break\n",
        "    print_values(V_pie)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a84d0dde"
      },
      "source": [
        "# reward matrix i\n",
        "def update_reward_matrix(reward):\n",
        "    reward_matrix = [[reward for _ in range(4)] for _ in range(3)]\n",
        "    reward_matrix[2][3] = 1\n",
        "    reward_matrix[1][3] = -1\n",
        "    return reward_matrix\n",
        "\n",
        "# V_pie initialization\n",
        "def initialize_V_pie():\n",
        "    V_pie = [[0 for _ in range(4)]for _ in range(3)]\n",
        "    return V_pie\n",
        "\n",
        "#printing matrix after convergence\n",
        "def print_values(V):\n",
        "  for i in range(2,-1,-1):\n",
        "    print(\" \")\n",
        "    for j in range(4):\n",
        "      v = V[i][j]\n",
        "      print(\" %.2f|\" % v, end=\"\")\n",
        "    print(\"\")"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2cfc8218",
        "outputId": "ed25ea96-2510-4b88-a2f4-06d52164a1ab"
      },
      "source": [
        "rewards = [-0.04,-2,0.1,0.02,1]\n",
        "epsilon = 1e-8\n",
        "discount_factor = 1\n",
        "print(\"Value Functions corresponding to optimal policy\\n\")\n",
        "for reward in rewards:\n",
        "    print(f\"For r(S) : {reward}\")\n",
        "    reward_matrix = update_reward_matrix(reward )\n",
        "    V_pie = initialize_V_pie()\n",
        "    iterative_policy_evaluation(0, epsilon, reward, reward_matrix, V_pie, discount_factor)\n",
        "    print(\"\\n\")"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Value Functions corresponding to optimal policy\n",
            "\n",
            "For r(S) : -0.04\n",
            "Number of iterations = 312\n",
            " \n",
            " -1.23| -0.83| -0.28| 0.00|\n",
            " \n",
            " -1.47| 0.00| -0.87| 0.00|\n",
            " \n",
            " -1.55| -1.47| -1.22| -1.17|\n",
            "\n",
            "\n",
            "For r(S) : -2\n",
            "Number of iterations = 384\n",
            " \n",
            " -59.71| -46.01| -24.32| 0.00|\n",
            " \n",
            " -65.41| 0.00| -21.94| 0.00|\n",
            " \n",
            " -63.10| -52.80| -34.49| -20.75|\n",
            "\n",
            "\n",
            "For r(S) : 0.1\n",
            "Number of iterations = 324\n",
            " \n",
            " 2.95| 2.39| 1.44| 0.00|\n",
            " \n",
            " 3.10| 0.00| 0.63| 0.00|\n",
            " \n",
            " 2.85| 2.20| 1.15| 0.23|\n",
            "\n",
            "\n",
            "For r(S) : 0.02\n",
            "Number of iterations = 284\n",
            " \n",
            " 0.56| 0.55| 0.46| 0.00|\n",
            " \n",
            " 0.49| 0.00| -0.23| 0.00|\n",
            " \n",
            " 0.34| 0.11| -0.20| -0.57|\n",
            "\n",
            "\n",
            "For r(S) : 1\n",
            "Number of iterations = 370\n",
            " \n",
            " 29.80| 23.14| 12.48| 0.00|\n",
            " \n",
            " 32.46| 0.00| 10.30| 0.00|\n",
            " \n",
            " 31.11| 25.77| 16.43| 9.22|\n",
            "\n",
            "\n"
          ]
        }
      ]
    }
  ]
}