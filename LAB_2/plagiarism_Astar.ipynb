{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gg-54HAyxdc_"
      },
      "outputs": [],
      "source": [
        "import heapq\n",
        "from collections import defaultdict\n",
        "import re\n",
        "import string\n",
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize\n",
        "import numpy as np\n",
        "from google.colab import drive\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "nltk.download('punkt')\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J0X6FwEKFA5T",
        "outputId": "51715151-e45d-4347-cf6e-028625909aa3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Text Preprocessing\n",
        "def preprocess_text(text):\n",
        "    text = text.lower()\n",
        "    sentences = sent_tokenize(text)\n",
        "    cleaned_sentences = [remove_punctuation(sentence) for sentence in sentences]\n",
        "    return cleaned_sentences\n",
        "\n",
        "def remove_punctuation(sentence):\n",
        "    return sentence.translate(str.maketrans(\"\\n\", \" \", string.punctuation))\n",
        "\n",
        "def read_file(file_path):\n",
        "    with open(file_path, \"r\") as file:\n",
        "        content = file.read()\n",
        "    return content\n"
      ],
      "metadata": {
        "id": "S0DhEnFeFFOO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# State Representation and A* Search\n",
        "class Node:\n",
        "    def __init__(self, state, parent=None):\n",
        "        self.state = state\n",
        "        self.parent = parent\n",
        "        self.g = 0\n",
        "        self.h = 0\n",
        "        self.f = 0\n",
        "\n",
        "    def __lt__(self, other):\n",
        "        return self.f < other.f\n",
        "\n",
        "def get_successors(node, doc1_len, doc2_len):\n",
        "    moves = [(1, 1, 0), (0, 1, 1), (1, 0, 2)]\n",
        "    successors = []\n",
        "    for move in moves:\n",
        "        new_state = (node.state[0] + move[0], node.state[1] + move[1], move[2])\n",
        "        if new_state[0] <= doc1_len and new_state[1] <= doc2_len:\n",
        "            successors.append(Node(new_state, node))\n",
        "    return successors"
      ],
      "metadata": {
        "id": "SDgDBI87FM4u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# semantic distance calculation\n",
        "def semantic_distance(sent1, sent2):\n",
        "    words1 = set(sent1.split())\n",
        "    words2 = set(sent2.split())\n",
        "    common_words = words1.intersection(words2)\n",
        "    return 1 - len(common_words) / max(len(words1), len(words2))\n",
        "\n",
        "# heuristic function\n",
        "def heuristic(state, doc1, doc2):\n",
        "    remaining_sentences1 = len(doc1) - state[0]\n",
        "    remaining_sentences2 = len(doc2) - state[1]\n",
        "    return abs(remaining_sentences1 - remaining_sentences2)\n",
        "\n",
        "\n",
        "# A* star search\n",
        "def a_star_search(doc1, doc2):\n",
        "    start_state = (0, 0, 0)\n",
        "    goal_state = (len(doc1), len(doc2), 0)\n",
        "    start_node = Node(start_state)\n",
        "    open_list = [start_node]\n",
        "    closed_set = set()\n",
        "\n",
        "    while open_list:\n",
        "        current_node = heapq.heappop(open_list)\n",
        "\n",
        "        if current_node.state[:2] == goal_state[:2]:\n",
        "            path = []\n",
        "            while current_node:\n",
        "                path.append(current_node.state)\n",
        "                current_node = current_node.parent\n",
        "            return path[::-1]\n",
        "\n",
        "        closed_set.add(current_node.state)\n",
        "\n",
        "        for successor in get_successors(current_node, len(doc1), len(doc2)):\n",
        "            if successor.state in closed_set:\n",
        "                continue\n",
        "\n",
        "            successor.g = current_node.g + semantic_distance(\n",
        "                doc1[successor.state[0]-1] if successor.state[0] > 0 else \"\",\n",
        "                doc2[successor.state[1]-1] if successor.state[1] > 0 else \"\"\n",
        "            )\n",
        "            successor.h = heuristic(successor.state, doc1, doc2)\n",
        "            successor.f = successor.g + successor.h\n",
        "\n",
        "            heapq.heappush(open_list, successor)\n",
        "\n",
        "    return None\n"
      ],
      "metadata": {
        "id": "p43oUEPkFTP0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plagiarism Detection with threshold 0.2\n",
        "def detect_plagiarism(doc1, doc2, threshold=0.2):\n",
        "    alignment = a_star_search(doc1, doc2)\n",
        "    plagiarism_detected = []\n",
        "\n",
        "    for i in range(1, len(alignment)):\n",
        "        prev, curr = alignment[i-1], alignment[i]\n",
        "        if curr[2] == 0:  # Sentences aligned\n",
        "            similarity = 1 - semantic_distance(doc1[curr[0]-1], doc2[curr[1]-1])\n",
        "            if similarity > threshold:\n",
        "                plagiarism_detected.append((curr[0]-1, curr[1]-1, similarity))\n",
        "\n",
        "    return plagiarism_detected"
      ],
      "metadata": {
        "id": "BtBvzebbFW0M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls /content/drive/MyDrive/Test/Test_1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y4Ag00dSIRQQ",
        "outputId": "fd173d0b-bdfe-4922-9604-4f11475fcc79"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "doc1.txt  doc2.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocesssing\n",
        "\n",
        "doc1 = preprocess_text(read_file(\"/content/drive/MyDrive/Test/Test_1/doc1.txt\"))\n",
        "doc2 = preprocess_text(read_file(\"/content/drive/MyDrive/Test/Test_1/doc2.txt\"))\n",
        "\n",
        "# Plagiarism detection\n",
        "plagiarism_results = detect_plagiarism(doc1, doc2)\n",
        "\n",
        "# Getting the results (similarity)\n",
        "print(\"Plagiarism Detection Results:\")\n",
        "for i, j, similarity in plagiarism_results:\n",
        "  print(f\"Document 1, Sentence {i+1}: {doc1[i]}\")\n",
        "  print(f\"Document 2, Sentence {j+1}: {doc2[j]}\")\n",
        "  print(f\"Similarity: {similarity:.2f}\")\n",
        "  print()\n",
        "\n",
        "# Final result\n",
        "print(f\"Total sentences in Document 1: {len(doc1)}\")\n",
        "print(f\"Total sentences in Document 2: {len(doc2)}\")\n",
        "print(f\"Number of potentially plagiarized sentences: {len(plagiarism_results)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jshxS6xBFcTW",
        "outputId": "c72098cb-c8e8-47ed-c783-e7986b3eb9dc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Plagiarism Detection Results:\n",
            "Document 1, Sentence 1: artificial intelligence is revolutionizing industries\n",
            "Document 2, Sentence 1: artificial intelligence is revolutionizing industries\n",
            "Similarity: 1.00\n",
            "\n",
            "Document 1, Sentence 2: machine learning algorithms can process vast amounts of data\n",
            "Document 2, Sentence 2: machine learning algorithms can process vast amounts of data\n",
            "Similarity: 1.00\n",
            "\n",
            "Document 1, Sentence 3: neural networks mimic the human brains structure\n",
            "Document 2, Sentence 3: neural networks mimic the human brains structure\n",
            "Similarity: 1.00\n",
            "\n",
            "Total sentences in Document 1: 3\n",
            "Total sentences in Document 2: 3\n",
            "Number of potentially plagiarized sentences: 3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocesssing\n",
        "doc1 = preprocess_text(read_file(\"/content/drive/MyDrive/Test/Test_2/doc1.txt\"))\n",
        "doc2 = preprocess_text(read_file(\"/content/drive/MyDrive/Test/Test_2/doc2.txt\"))\n",
        "\n",
        "# Plagiarism detection\n",
        "plagiarism_results = detect_plagiarism(doc1, doc2)\n",
        "\n",
        "# Getting the results (similarity)\n",
        "print(\"Plagiarism Detection Results:\")\n",
        "for i, j, similarity in plagiarism_results:\n",
        "  print(f\"Document 1, Sentence {i+1}: {doc1[i]}\")\n",
        "  print(f\"Document 2, Sentence {j+1}: {doc2[j]}\")\n",
        "  print(f\"Similarity: {similarity:.2f}\")\n",
        "  print()\n",
        "\n",
        "# Final result\n",
        "print(f\"Total sentences in Document 1: {len(doc1)}\")\n",
        "print(f\"Total sentences in Document 2: {len(doc2)}\")\n",
        "print(f\"Number of potentially plagiarized sentences: {len(plagiarism_results)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oGWVluhFJ-2q",
        "outputId": "98619637-fe7f-4086-c050-93cd2e3c3399"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Plagiarism Detection Results:\n",
            "Document 1, Sentence 1: quantum computation utilizes quantum physics concepts\n",
            "Document 2, Sentence 1: quantum computation utilizes quantum physics concepts\n",
            "Similarity: 1.00\n",
            "\n",
            "Document 1, Sentence 2: quantum bits can be in various states at once\n",
            "Document 2, Sentence 2: quantum bits can be in various states at once\n",
            "Similarity: 1.00\n",
            "\n",
            "Document 1, Sentence 3: this innovation may tackle intricate issues at unprecedented speeds\n",
            "Document 2, Sentence 3: this innovation may tackle intricate issues at unprecedented speeds\n",
            "Similarity: 1.00\n",
            "\n",
            "Total sentences in Document 1: 3\n",
            "Total sentences in Document 2: 3\n",
            "Number of potentially plagiarized sentences: 3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocesssing\n",
        "doc1 = preprocess_text(read_file(\"/content/drive/MyDrive/Test/Test_3/doc1.txt\"))\n",
        "doc2 = preprocess_text(read_file(\"/content/drive/MyDrive/Test/Test_3/doc2.txt\"))\n",
        "\n",
        "# Plagiarism detection\n",
        "plagiarism_results = detect_plagiarism(doc1, doc2)\n",
        "\n",
        "# Getting the results (similarity)\n",
        "print(\"Plagiarism Detection Results:\")\n",
        "for i, j, similarity in plagiarism_results:\n",
        "  print(f\"Document 1, Sentence {i+1}: {doc1[i]}\")\n",
        "  print(f\"Document 2, Sentence {j+1}: {doc2[j]}\")\n",
        "  print(f\"Similarity: {similarity:.2f}\")\n",
        "  print()\n",
        "\n",
        "# Final result\n",
        "print(f\"Total sentences in Document 1: {len(doc1)}\")\n",
        "print(f\"Total sentences in Document 2: {len(doc2)}\")\n",
        "print(f\"Number of potentially plagiarized sentences: {len(plagiarism_results)}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xDEXEofWKCUe",
        "outputId": "01f5fe40-acc2-4707-9c12-981601bb8fd6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Plagiarism Detection Results:\n",
            "Total sentences in Document 1: 3\n",
            "Total sentences in Document 2: 3\n",
            "Number of potentially plagiarized sentences: 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocesssing\n",
        "doc1 = preprocess_text(read_file(\"/content/drive/MyDrive/Test/Test_4/doc1.txt\"))\n",
        "doc2 = preprocess_text(read_file(\"/content/drive/MyDrive/Test/Test_4/doc2.txt\"))\n",
        "\n",
        "# Plagiarism detection\n",
        "plagiarism_results = detect_plagiarism(doc1, doc2)\n",
        "\n",
        "# Getting the results (similarity)\n",
        "print(\"Plagiarism Detection Results:\")\n",
        "for i, j, similarity in plagiarism_results:\n",
        "  print(f\"Document 1, Sentence {i+1}: {doc1[i]}\")\n",
        "  print(f\"Document 2, Sentence {j+1}: {doc2[j]}\")\n",
        "  print(f\"Similarity: {similarity:.2f}\")\n",
        "  print()\n",
        "\n",
        "# Final result\n",
        "print(f\"Total sentences in Document 1: {len(doc1)}\")\n",
        "print(f\"Total sentences in Document 2: {len(doc2)}\")\n",
        "print(f\"Number of potentially plagiarized sentences: {len(plagiarism_results)}\")"
      ],
      "metadata": {
        "id": "zBpVT3frKGF-",
        "outputId": "0e814ce8-7f61-40d3-ad9d-f15ed67c95e7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Plagiarism Detection Results:\n",
            "Document 1, Sentence 1: natural language processing nlp allows computers to understand human language\n",
            "Document 2, Sentence 1: natural language processing nlp allows computers to understand human language\n",
            "Similarity: 1.00\n",
            "\n",
            "Total sentences in Document 1: 3\n",
            "Total sentences in Document 2: 3\n",
            "Number of potentially plagiarized sentences: 1\n"
          ]
        }
      ]
    }
  ]
}