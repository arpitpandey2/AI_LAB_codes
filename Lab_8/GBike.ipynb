{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "763e775f"
      },
      "source": [
        "import numpy as np\n",
        "from scipy.stats import poisson\n",
        "\n",
        "\n",
        "# poisson distribution setup\n",
        "class Poisson:\n",
        "    cache_pmf = {}\n",
        "    cache_sf = {}\n",
        "    cache = {}\n",
        "    MAX_CUTOFF = 25\n",
        "\n",
        "    @classmethod\n",
        "    def pmf_series(cls, mu, cutoff):\n",
        "        assert isinstance(mu, int), \"mu should be an integer.\"\n",
        "        assert isinstance(cutoff, int), \"cutoff should be an integer\"\n",
        "\n",
        "        if (mu, cutoff) not in cls.cache:\n",
        "            cls._calculate_pmf_series(mu, cutoff)\n",
        "\n",
        "        return cls.cache[(mu, cutoff)]\n",
        "\n",
        "    @classmethod\n",
        "    def _calculate_pmf_series(cls, mu, cutoff):\n",
        "        if mu not in cls.cache_pmf:\n",
        "            cls.cache_pmf[mu] = poisson.pmf(np.arange(cls.MAX_CUTOFF + 1), mu)\n",
        "            cls.cache_sf[mu] = poisson.sf(np.arange(cls.MAX_CUTOFF + 1), mu)\n",
        "\n",
        "        out = np.copy(cls.cache_pmf[mu][:cutoff+1])\n",
        "        out[-1] += cls.cache_sf[mu][cutoff]\n",
        "\n",
        "        cls.cache[(mu, cutoff)] = out"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "48534928"
      },
      "source": [
        "# policy iteration for G-bike problem\n",
        "class PolicyIterationSolver1:\n",
        "    capacity = 20\n",
        "    rental_reward = 10.\n",
        "    moving_cost = 2.\n",
        "    max_moving = 5\n",
        "    bad_action_cost = 100.\n",
        "\n",
        "    request_mean_G1 = 3\n",
        "    request_mean_G2 = 4\n",
        "    return_mean_G1 = 3\n",
        "    return_mean_G2 = 2\n",
        "\n",
        "    discount = 0.9\n",
        "    PolicyEvaluationError = 0.01\n",
        "\n",
        "    def init(self):\n",
        "        self.policy = np.zeros([self.capacity + 1]*2, int)\n",
        "        self.value = np.zeros([self.capacity + 1]*2)\n",
        "\n",
        "        self._reward1 = self.expected_rental_reward(self.request_mean_G1)\n",
        "        self._reward2 = self.expected_rental_reward(self.request_mean_G2)\n",
        "\n",
        "        assert self.bad_action_cost >= 0\n",
        "\n",
        "    def bellman(self, action, s1, s2):\n",
        "        # Transition probabilities for each location\n",
        "        transp1 = self.transition_probabilty(s1, self.request_mean_G1, self.return_mean_G1, -action)\n",
        "        transp2 = self.transition_probabilty(s2, self.request_mean_G2, self.return_mean_G2, action)\n",
        "        transp = np.outer(transp1, transp2)\n",
        "\n",
        "        # Calculate total reward and expected value\n",
        "        return self._reward1[s1] + self._reward2[s2] - self.expected_moving_cost(s1, s2, action) + \\\n",
        "               self.discount * sum((transp * self.value).flat)\n",
        "\n",
        "    def policy_evaluation(self):\n",
        "        while True:\n",
        "            diff = 0.\n",
        "            it = np.nditer([self.policy], flags=['multi_index'])\n",
        "\n",
        "            while not it.finished:\n",
        "                action = it[0]\n",
        "                s1, s2 = it.multi_index\n",
        "\n",
        "                _temp = self.value[s1, s2]\n",
        "                self.value[s1, s2] = self.bellman(action=action, s1=s1, s2=s2)\n",
        "\n",
        "                diff = max(diff, abs(self.value[s1, s2] - _temp))\n",
        "                it.iternext()\n",
        "\n",
        "            if diff < self.PolicyEvaluationError:\n",
        "                break\n",
        "\n",
        "    def policy_update(self):\n",
        "        is_policy_changed = False\n",
        "        it = np.nditer([self.policy], flags=['multi_index'])\n",
        "\n",
        "        while not it.finished:\n",
        "            s1, s2 = it.multi_index\n",
        "\n",
        "            _max_val = -1\n",
        "            _pol = None\n",
        "\n",
        "            for act in range(-self.max_moving, self.max_moving + 1):\n",
        "                _val = self.bellman(action=act, s1=s1, s2=s2)\n",
        "                if _val > _max_val:\n",
        "                    _max_val = _val\n",
        "                    _pol = act\n",
        "\n",
        "            if self.policy[s1, s2] != _pol:\n",
        "                is_policy_changed = True\n",
        "                self.policy[s1, s2] = _pol\n",
        "\n",
        "            it.iternext()\n",
        "\n",
        "        return is_policy_changed\n",
        "\n",
        "    def expected_moving_cost(self, s1, s2, action):\n",
        "        if action == 0:\n",
        "            return 0.\n",
        "\n",
        "        # moving from state s1 into state s2\n",
        "        if action > 0:\n",
        "            p = self.transition_probabilty(s1, self.request_mean_G1, self.return_mean_G1)\n",
        "            cost = self._gen_move_cost_array(action)\n",
        "            return cost.dot(p)\n",
        "\n",
        "        # moving from state s2 into state s1\n",
        "        p = self.transition_probabilty(s2, self.request_mean_G2, self.return_mean_G2)\n",
        "        cost = self._gen_move_cost_array(action)\n",
        "        return cost.dot(p)\n",
        "\n",
        "    def _gen_move_cost_array(self, action):\n",
        "        _action = abs(action)\n",
        "\n",
        "        # Don't punish bad action:\n",
        "        if self.bad_action_cost == 0:\n",
        "            cost = np.asarray(\n",
        "                [ii if ii < _action else _action for ii in range(self.capacity+1)]\n",
        "            ) * self.moving_cost\n",
        "\n",
        "        # bad action is punished\n",
        "        else:\n",
        "            cost = np.asarray(\n",
        "                [self.bad_action_cost if ii < _action else _action for ii in range(self.capacity + 1)]\n",
        "            ) * self.moving_cost\n",
        "        return cost\n",
        "\n",
        "    @classmethod\n",
        "    def expected_rental_reward(cls, expected_request):\n",
        "        return np.asarray([cls._state_reward(s, expected_request) for s in range(cls.capacity + 1)])\n",
        "\n",
        "    @classmethod\n",
        "    def _state_reward(cls, s, mu):\n",
        "        rewards = cls.rental_reward * np.arange(s + 1)\n",
        "        p = Poisson.pmf_series(mu, cutoff=s)\n",
        "        return rewards.dot(p)\n",
        "\n",
        "    def transition_probabilty(self, s, req, ret, action=0):\n",
        "        _ret_sz = self.max_moving + self.capacity\n",
        "\n",
        "        p_req = Poisson.pmf_series(req, s)\n",
        "        p_ret = Poisson.pmf_series(ret, _ret_sz)\n",
        "        p = np.outer(p_req, p_ret)\n",
        "\n",
        "        transp = np.asarray([p.trace(offset) for offset in range(-s, _ret_sz + 1)])\n",
        "\n",
        "        assert abs(action) <= self.max_moving, f\"action can be larger than {self.max_moving}.\"\n",
        "\n",
        "        # No GBikes are being moved\n",
        "        if action == 0:\n",
        "            transp[20] += sum(transp[21:])\n",
        "            return transp[:21]\n",
        "\n",
        "        # Move GBikes from station 1 to station 2\n",
        "        if action > 0:\n",
        "            transp[self.capacity-action] += sum(transp[self.capacity-action+1:])\n",
        "            transp[self.capacity-action+1:] = 0\n",
        "\n",
        "            return np.roll(transp, shift=action)[:self.capacity+1]\n",
        "\n",
        "        # Move GBikes from station 2 to station 1\n",
        "        action = -action\n",
        "        transp[action] += sum(transp[:action])\n",
        "        transp[:action] = 0\n",
        "\n",
        "        transp[action+self.capacity] += sum(transp[action+self.capacity+1:])\n",
        "        transp[action+self.capacity+1:] = 0\n",
        "\n",
        "        return np.roll(transp, shift=-action)[:self.capacity+1]\n",
        "\n",
        "    def policy_iteration(self):\n",
        "        self.policy_evaluation()\n",
        "        while self.policy_update():\n",
        "            self.policy_evaluation()"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "22b4ec94"
      },
      "source": [
        "# policy iteration for modified G-bike problem\n",
        "class PolicyIterationSolver2:\n",
        "    capacity = 20\n",
        "    rental_reward = 10.\n",
        "    moving_cost = 2.\n",
        "    max_moving = 5\n",
        "    bad_action_cost = 100.\n",
        "\n",
        "    # New parking lot cost\n",
        "    parking_lot_cost = 4.\n",
        "\n",
        "    request_mean_G1 = 3\n",
        "    request_mean_G2 = 4\n",
        "    return_mean_G1 = 3\n",
        "    return_mean_G2 = 2\n",
        "\n",
        "    discount = 0.9\n",
        "    PolicyEvaluationError = 0.01\n",
        "\n",
        "    def init(self):\n",
        "        self.policy = np.zeros([self.capacity + 1]*2, int)\n",
        "        self.value = np.zeros([self.capacity + 1]*2)\n",
        "\n",
        "        self._reward1 = self.expected_rental_reward(self.request_mean_G1)\n",
        "        self._reward2 = self.expected_rental_reward(self.request_mean_G2)\n",
        "\n",
        "        assert self.bad_action_cost >= 0\n",
        "\n",
        "    def bellman(self, action, s1, s2):\n",
        "        # Transition probabilities for each location\n",
        "        transp1 = self.transition_probabilty(s1, self.request_mean_G1, self.return_mean_G1, -action)\n",
        "        transp2 = self.transition_probabilty(s2, self.request_mean_G2, self.return_mean_G2, action)\n",
        "        transp = np.outer(transp1, transp2)\n",
        "\n",
        "        # Calculate total reward and expected value\n",
        "        return self._reward1[s1] + self._reward2[s2] - self.expected_moving_cost(s1, s2, action) - self.parking_lot_penalty(s1, s2, action) + \\\n",
        "               self.discount * sum((transp * self.value).flat)\n",
        "\n",
        "    def parking_lot_penalty(self, s1, s2, action):\n",
        "        # Calculate final bike count at each location after moving\n",
        "        new_s1 = max(0, min(self.capacity, s1 - action))\n",
        "        new_s2 = max(0, min(self.capacity, s2 + action))\n",
        "\n",
        "        # Parking lot penalty calculation\n",
        "        penalty = 0\n",
        "\n",
        "        # Penalty for first location (s1)\n",
        "        if new_s1 > 10:\n",
        "            penalty += self.parking_lot_cost\n",
        "\n",
        "        # Penalty for second location (s2)\n",
        "        if new_s2 > 10:\n",
        "            penalty += self.parking_lot_cost\n",
        "\n",
        "        # Adjust moving cost for first free bike shuttle\n",
        "        if action == 1:\n",
        "            penalty -= self.moving_cost\n",
        "        elif action == -1:\n",
        "            penalty -= self.moving_cost\n",
        "\n",
        "        return penalty\n",
        "\n",
        "    def policy_evaluation(self):\n",
        "        while True:\n",
        "            diff = 0.\n",
        "            it = np.nditer([self.policy], flags=['multi_index'])\n",
        "\n",
        "            while not it.finished:\n",
        "                action = it[0]\n",
        "                s1, s2 = it.multi_index\n",
        "\n",
        "                _temp = self.value[s1, s2]\n",
        "                self.value[s1, s2] = self.bellman(action=action, s1=s1, s2=s2)\n",
        "\n",
        "                diff = max(diff, abs(self.value[s1, s2] - _temp))\n",
        "                it.iternext()\n",
        "\n",
        "            if diff < self.PolicyEvaluationError:\n",
        "                break\n",
        "\n",
        "    def policy_update(self):\n",
        "        is_policy_changed = False\n",
        "        it = np.nditer([self.policy], flags=['multi_index'])\n",
        "\n",
        "        while not it.finished:\n",
        "            s1, s2 = it.multi_index\n",
        "\n",
        "            _max_val = -float('inf')\n",
        "            _pol = None\n",
        "\n",
        "            for act in range(-self.max_moving, self.max_moving + 1):\n",
        "                _val = self.bellman(action=act, s1=s1, s2=s2)\n",
        "                if _val > _max_val:\n",
        "                    _max_val = _val\n",
        "                    _pol = act\n",
        "\n",
        "            if self.policy[s1, s2] != _pol:\n",
        "                is_policy_changed = True\n",
        "                self.policy[s1, s2] = _pol\n",
        "\n",
        "            it.iternext()\n",
        "\n",
        "        return is_policy_changed\n",
        "\n",
        "    def expected_moving_cost(self, s1, s2, action):\n",
        "        if action == 0:\n",
        "            return 0.\n",
        "\n",
        "        # moving from state s1 into state s2\n",
        "        if action > 0:\n",
        "            p = self.transition_probabilty(s1, self.request_mean_G1, self.return_mean_G1)\n",
        "            cost = self._gen_move_cost_array(action)\n",
        "            return cost.dot(p)\n",
        "\n",
        "        # moving from state s2 into state s1\n",
        "        p = self.transition_probabilty(s2, self.request_mean_G2, self.return_mean_G2)\n",
        "        cost = self._gen_move_cost_array(action)\n",
        "        return cost.dot(p)\n",
        "\n",
        "    def _gen_move_cost_array(self, action):\n",
        "        _action = abs(action)\n",
        "\n",
        "        # Don't punish bad action:\n",
        "        if self.bad_action_cost == 0:\n",
        "            cost = np.asarray(\n",
        "                [ii if ii < _action else _action for ii in range(self.capacity+1)]\n",
        "            ) * self.moving_cost\n",
        "\n",
        "        # bad action is punished\n",
        "        else:\n",
        "            cost = np.asarray(\n",
        "                [self.bad_action_cost if ii < _action else _action for ii in range(self.capacity + 1)]\n",
        "            ) * self.moving_cost\n",
        "        return cost\n",
        "\n",
        "    @classmethod\n",
        "    def expected_rental_reward(cls, expected_request):\n",
        "        return np.asarray([cls._state_reward(s, expected_request) for s in range(cls.capacity + 1)])\n",
        "\n",
        "    @classmethod\n",
        "    def _state_reward(cls, s, mu):\n",
        "        rewards = cls.rental_reward * np.arange(s + 1)\n",
        "        p = Poisson.pmf_series(mu, cutoff=s)\n",
        "        return rewards.dot(p)\n",
        "\n",
        "    def transition_probabilty(self, s, req, ret, action=0):\n",
        "        _ret_sz = self.max_moving + self.capacity\n",
        "\n",
        "        p_req = Poisson.pmf_series(req, s)\n",
        "        p_ret = Poisson.pmf_series(ret, _ret_sz)\n",
        "        p = np.outer(p_req, p_ret)\n",
        "\n",
        "        transp = np.asarray([p.trace(offset) for offset in range(-s, _ret_sz + 1)])\n",
        "\n",
        "        assert abs(action) <= self.max_moving, f\"action can be larger than {self.max_moving}.\"\n",
        "\n",
        "        # No GBikes are being moved\n",
        "        if action == 0:\n",
        "            transp[20] += sum(transp[21:])\n",
        "            return transp[:21]\n",
        "\n",
        "        # Move GBikes from station 1 to station 2\n",
        "        if action > 0:\n",
        "            transp[self.capacity-action] += sum(transp[self.capacity-action+1:])\n",
        "            transp[self.capacity-action+1:] = 0\n",
        "\n",
        "            return np.roll(transp, shift=action)[:self.capacity+1]\n",
        "\n",
        "        # Move GBikes from station 2 to station 1\n",
        "        action = -action\n",
        "        transp[action] += sum(transp[:action])\n",
        "        transp[:action] = 0\n",
        "\n",
        "        transp[action+self.capacity] += sum(transp[action+self.capacity+1:])\n",
        "        transp[action+self.capacity+1:] = 0\n",
        "\n",
        "        return np.roll(transp, shift=-action)[:self.capacity+1]\n",
        "\n",
        "    def policy_iteration(self):\n",
        "        self.policy_evaluation()\n",
        "        while self.policy_update():\n",
        "            self.policy_evaluation()"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "43dd5c40",
        "outputId": "778125fe-c4b1-40cb-8913-fbc7dbbec035"
      },
      "source": [
        "solver1 = PolicyIterationSolver1()\n",
        "solver1.init()\n",
        "solver1.policy_iteration()\n",
        "\n",
        "print(\"Part 1 Policy:\")\n",
        "print(solver1.policy)\n",
        "# print(\"\\nPart 1 Value:\")\n",
        "# print(solver1.value)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Part 1 Policy:\n",
            "[[ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 -1 -1 -1 -2]\n",
            " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 -1 -1 -1 -1]\n",
            " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 -1 -1 -1]\n",
            " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 -1 -1]\n",
            " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 -1]\n",
            " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
            " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
            " [ 1  1  1  1  1  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
            " [ 1  1  1  1  1  1  1  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
            " [ 2  2  2  2  2  2  1  1  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
            " [ 3  3  3  3  2  2  2  1  1  0  0  0  0  0  0  0  0  0  0  0  0]\n",
            " [ 3  3  3  3  3  3  2  2  1  0  0  0  0  0  0  0  0  0  0  0  0]\n",
            " [ 4  4  4  4  3  3  3  2  1  1  0  0  0  0  0  0  0  0  0  0  0]\n",
            " [ 4  4  4  4  4  3  3  2  2  1  0  0  0  0  0  0  0  0  0  0  0]\n",
            " [ 5  4  4  4  4  4  3  3  2  1  0  0  0  0  0  0  0  0  0  0  0]\n",
            " [ 5  5  5  5  4  4  3  3  2  1  1  0  0  0  0  0  0  0  0  0  0]\n",
            " [ 5  5  5  5  5  4  4  3  2  2  1  0  0  0  0  0  0  0  0  0  0]\n",
            " [ 5  5  5  5  5  4  4  3  3  2  1  1  0  0  0  0  0  0  0  0  0]\n",
            " [ 5  5  5  5  5  4  4  3  3  2  2  1  1  1  0  0  0  0  0  0  0]\n",
            " [ 5  5  5  5  5  5  4  4  3  3  2  2  2  1  1  1  1  0  0  0  0]\n",
            " [ 5  5  5  5  5  5  5  4  4  3  3  3  2  2  2  2  1  1  0  0  0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "07418ec3",
        "outputId": "ba7a4a01-09e4-4d60-c41f-2834480a88c7"
      },
      "source": [
        "print()\n",
        "print(\"-\"*70)\n",
        "print()\n",
        "\n",
        "solver2 = PolicyIterationSolver2()\n",
        "solver2.init()\n",
        "solver2.policy_iteration()\n",
        "\n",
        "print(\"Part 2 Policy:\")\n",
        "print(solver2.policy)\n",
        "# print(\"\\nPart 2 Value:\")\n",
        "# print(solver2.value)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "----------------------------------------------------------------------\n",
            "\n",
            "Part 2 Policy:\n",
            "[[ 0  0  0  0  0  0  0  0  0  0 -1 -1 -2 -1 -1 -1 -1 -1 -1 -1 -1]\n",
            " [ 0  0  0  0  0  0  0  0  0  0 -1 -1 -2 -1 -1 -1 -1 -1 -1 -1 -1]\n",
            " [ 0  0  0  0  0  0  0  0  0  0 -1 -1 -2 -1 -1 -1 -1 -1 -1 -1 -1]\n",
            " [ 0  0  0  0  0  0  0  0  0  0 -1 -1 -2 -1 -1 -1 -1 -1 -1 -1 -1]\n",
            " [ 0  0  0  0  0  0  0  0  0  0  0 -1 -2 -1 -1 -1 -1 -1 -1 -1 -1]\n",
            " [ 0  0  0  0  0  0  0  0  0  0  0 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1]\n",
            " [ 1  1  1  1  1  1  1  1  0  0  0 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1]\n",
            " [ 1  1  1  1  1  1  1  1  1  1  0 -1  0 -1 -1 -1 -1 -1 -1 -1 -1]\n",
            " [ 1  1  1  1  1  1  1  1  1  1  0 -1  0  0  0 -1 -1  0 -1 -1 -1]\n",
            " [ 1  1  1  1  1  1  1  1  1  1  0 -1  1  1  0  0  0  0  0  0  0]\n",
            " [ 1  1  1  1  1  1  1  1  1  1  0  1  1  1  1  1  1  1  1  1  0]\n",
            " [ 1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1]\n",
            " [ 3  3  3  3  3  3  2  2  2  1  0 -1  2  2  2  2  2  2  2  2  2]\n",
            " [ 4  4  4  4  3  3  3  3  1  1  0 -1  1  1  1  1  1  1  1  1  1]\n",
            " [ 4  4  4  4  4  4  4  1  1  1  0 -1  1  1  1  1  1  1  1  1  1]\n",
            " [ 5  5  5  5  5  5  1  1  1  1  0 -1  1  1  1  1  1  1  1  1  1]\n",
            " [ 5  5  5  5  5  4  1  1  1  1  0 -1  1  1  1  1  1  1  1  1  1]\n",
            " [ 5  5  5  5  5  4  1  1  1  1  0 -1  1  1  1  1  1  1  1  1  1]\n",
            " [ 5  5  5  5  5  4  4  1  1  1  0 -1  1  1  1  1  1  1  1  1  1]\n",
            " [ 5  5  5  5  5  4  4  1  1  1  0 -1  1  1  1  1  1  1  1  1  1]\n",
            " [ 5  5  5  5  5  5  4  1  1  1  0  1  1  1  1  1  1  1  1  1  1]]\n"
          ]
        }
      ]
    }
  ]
}